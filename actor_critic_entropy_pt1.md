### Actor-Critic Algorithm Explanation
The Actor-Critic algorithm implemented in policy.py represents a hybrid reinforcement learning approach that elegantly combines policy-based and value-based methods for the challenging rocket landing task. At its foundation are two neural networks working in concert: the Actor network, which outputs probabilities over possible actions through a softmax activation, and the Critic network, which evaluates state values to guide the learning process. Both networks leverage positional mapping inspired by NeRF techniques, transforming low-dimensional state inputs into richer representations capable of capturing complex spatial relationships critical for precise rocket control.

The architecture's learning mechanism revolves around the update_ac() method, which orchestrates the interplay between the Actor and Critic. When called, this method first calculates n-step returns using discounted rewards collected from the environment. It then derives advantage estimates by comparing these empirical returns against the Critic's predictions, essentially measuring how much better each action performed than expected. The Actor's policy is improved through gradient ascent on the log probabilities of chosen actions, weighted by their respective advantages - actions that yielded better-than-expected outcomes are reinforced. Simultaneously, the Critic refines its value estimates by minimizing the mean squared error between its predictions and the actual returns, gradually building a more accurate evaluation function.

I've enhanced this implementation with entropy regularization to address premature convergence issues. This addition encourages exploration by rewarding policies with higher entropy (more uniform action distributions) through a small bonus term in the loss function. The implementation calculates entropy as -sum(p * log(p)) across the action distribution and subtracts a scaled version from the overall loss, effectively creating a soft constraint that penalizes overly deterministic behavior. This modification draws inspiration from Soft Actor-Critic methodology, creating a balance between exploiting known good strategies and continuing to explore potentially better alternatives. Practically, this helps the rocket avoid getting stuck in local optima, particularly important when learning to navigate the complex dynamics of controlled descent and precision landing. 